# Кодировки

## Введение

Работая со строками обычно редко задумываются как они хранятся и что из себя представляют на самом деле.

Все с чем вы работаете на компьютере хранится в виде последовательности битов. Бит имеет два значения: 1 или 0.

Так вот каждому символу, который вы видите на экране, соответствует определенная комбинация нулей и единиц — его код.

Совокупность таких кодов называется кодировкой.

К примеру:

```java
01100010 01101001 01110100 01110011
bits
```

В этой кодировке `01100010` представляет из себя `b`, представлению `i`соответствует `01101001`, `01110100` — это `t`, ну и `01110011` — это `s`.

Внешний вид самих символов определяется файлами шрифтов, которые предоставлены операционной системой.

Поэтому процесс вывода на экран текста можно описать как постоянное сопоставление последовательностей нулей и единиц каким-то конкретным символам, входящим в состав шрифта.

Кодировок существует огромное множество и для того, чтобы понять почему так произошло, необходимо обратиться к истории.

## История

### ASCII

Прародителем всех современных кодировок можно считать [ASCII](https://ru.wikipedia.org/wiki/ASCII).

Эта аббревиатура расшифровывается как `American Standard Code for Information Interchange`: американская стандартная кодировочная таблица для печатных символов и некоторых специальных кодов.

Кодировка `ASCII` была разработана в `1963` году Американской Ассоциацией Стандартов (которая позже стала Американским Национальным Институтом Стандартов — `ANSI`). Кодировка несколько раз обновлялась — в 1967 и 1986 годах.

Изначально `ASCII` являлась 7-битной кодировкой, включающая в себя 128 символов: 33 непечатных управляющих символа (влияющих на обработку текста и пробелов) и 95 печатных символов, включая цифры, буквы латинского алфавита в строчном и прописном вариантах, ряд пунктуационных символов.

> Не использование 8-го бита было связано с тем, что кодировка разрабатывалась для обмена информацией по телетайпу и старший бит использовался для контроля ошибок, возникших при передаче данных. 

Позже ее расширили для использования 8-го бита и появилась возможность использования различных символов, которые можно закодировать в одном байте информации.

Такое усовершенствование позволило добавлять в `ASCII` символы национальных языков, помимо уже существующей латиницы.

А так как языков в мире огромное количество, то и вариантов расширенной кодировки `ASCII` существует огромное количество. Например, `KOI8-R` - это тоже расширенная кодировка `ASCII`.

Кодировка `KOI8` была разработана в СССР в 1974 году.

Аббревиатура `KOI8` расшифровывается как Код Обмена Информацией, 8 бит. Как следует из названия, это была 8-битная кодировка, что позволяло включить в нее в два раза больше символов, чем вмещала 7-битная версия `ASCII`. Кодировака `KOI8` включала в себя цифры, буквы латинского и русского алфавита, а также знаки пунктуации, спецсимволы и псевдографику.

Кодировка `KOI8-R` была предназначена для русского алфавита, вариант `KOI8-U` — для украинского.

### ANSI

Следующим шагом в развитии кодировок было появление так называемых `ANSI`-кодировок.

По сути это были те же расширенные версии `ASCII`, однако из них были удалены различные псевдографические элементы и добавлены символы типографики, для которых ранее не хватало 'свободных мест'.

В это время были разработаны такие известные кодировки как `Windows-1251`, `Windows-1252` и т.д. 

До этого момента включительно человечество все еще следовало правилу `1 символ - 1 байт` и пыталось вместить в 8 бит все необходимое.

Все это привело к тому, что появилось большое количество кодировок в которых коды кириллических символов были разными.

> Например, в KOI8-R русский алфавит был расположен не в алфавитном порядке: позиции символов русского алфавита соответствовали их фонетическим аналогам в английском алфавите.

Из-за отсутствия общего стандарта часто возникала проблема отображения символов, так как символы были закодированны с помощью одной кодировочной таблицы, а отображение происходило с помощью другой кодировочной таблицы.

Все эти проблемы и сложности подтолкнули к созданию некой универсальной кодировки, способной вместить в себя все необходимые символы.

### Unicode

Стандарт [Unicode](https://ru.wikipedia.org/wiki/%D0%AE%D0%BD%D0%B8%D0%BA%D0%BE%D0%B4) был предложен некоммерческой организацией 'Консорциум Юникода'(Unicode Consortium, Unicode Inc.) в 1991 году.

В самом начале было предложено для кодирования одного символа предполагалось использовать аж 32 бита. И такая кодировка получила название [UTF-32](https://ru.wikipedia.org/wiki/UTF-32).

Аббревиатура `UTF` расшифровывается как Unicode Transformation Format.

Ясно, что с помощью такой кодировки можно представить огромное количество символов, однако за такой размер необходимо платить тем, что каждый символ занимает 4 байта.

Во времена модемной связи и малых объемов устройств хранения информации это была непозволительная роскошь.

Понимая это, была разработана кодировка `UTF-16`, из названия которой понятно, что используется 16 бит.

Количества символов хватало для комфортной работы, но проблема заключалась в том, что при переходе на эту кодировку с `ASCII` размер файлов увеличивался вдвое. Поэтому трудно было убедить всех перейти на общий стандарт.

Учтя все недочеты была предпринята еще одна попытка перевести всех к одному стандарту, в результате чего была разработана многобайтовая кодировка с переменной длинной символа: `UTF-8`.

### UTF-8

В [UTF-8](https://ru.wikipedia.org/wiki/UTF-8) все латинские символы кодируются 8 битами, как и в кодировке `ASCII`. Базовая часть кодировки `ASCII` - 128 символов - перешла в `UTF-8`, что позволяет хранить их в 1 байте, сохраняя при этом универсальность, ради которой все и затевалось.

Каждый символ кодировки, отличный от `ASCII`, состоит из ведущего байта, указывающего длину последовательности, и одного или нескольких продолжающих байт. Такой принцип позволяет определить длину последовательности только по первому байту

Поэтому, первые 128 символов кодируются 1 байтом, в то время как все остальные символы кодируются уже 2 байтами и более.

В частности, каждый символ кириллицы кодируется именно 2 байтами.

Стандарт `RFC-3629` ограничивает `UTF-8` 4 байтами, однако теоретически `UTF-8` позволяет использовать последовательности вплоть до 6 байт.

Кодировка `UTF-8` является универсальной и имеет внушительный резерв на будущее. Это делает ее наиболее удобной кодировкой для использования.

## BOM

`BOM` расшифровывается как Byte Order Mark. Это специальный `Unicode`-символ, используемый для индикации порядка байтов текстового файла.

Вставляется в начало текстового файла или потока для обозначения того, что в файле/потоке используется `Unicode`, а также для косвенного указания кодировки и порядка байтов, с помощью которых символы были закодированы. Номер этого символа в стандарте Юникод — `U+FEFF`.

По сути это метаинформация о кодировке, записывающаяся в начало файла.

Использование `BOM` **не является** обязательным, поэтому встречается он редко.

Символ является непечатным и иногда может вызвать некоторые проблемы, например, он может быть воспринят как символ, занимающий целую строку.

Подробнее про [BOM](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%80%D0%BA%D0%B5%D1%80_%D0%BF%D0%BE%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D0%B1%D0%B0%D0%B9%D1%82%D0%BE%D0%B2).

## Кодировки в Java

По-умолчанию `JVM` использует `UTF-8` кодировку. 
Однако это поведение может быть изменено с помощью параметра file.encoding(например, `-Dfile.encoding=UTF-16`) при запуске `JVM`, либо явным заданием этого параметра:

```java
System.setProperty("file.encoding","UTF-8");
```

Тип данных `char` в `Java` занимает **2 байта**.

Тип данных `java.lang.String` - это массив `char` символов.

Для демонстрации этого выведем на экран количество `char` символов в строке 'Hello World':

```java
System.out.println("Hello World".length());
```

Что выведет на экран `11`. 

---

**Вопрос**:

Но существуют же символы, которые занимают в кодировке, которую использует `Java`, более 2 байт!

Например:

```java
𝔊 - MATHEMATICAL_FRAKTUR_CAPITAL_G
```

Как в таком случае быть?

**Ответ**:

Такие символы занимают в `Java` два `char`-а.

При этом `Java` сразу заменит его на шестнадцатеричный номер в `unicode`:

```java
public class Test {
    public static void main(String[] args) {
        System.out.println("\uD835\uDD0A");
        System.out.println("\uD835\uDD0A".length());
    }
}
```

Что выведет на экран:

```java
𝔊
2
```
---

Из примера видно, что количество `char` символов может быть одно, а количество `unicode`-символов в строке - уже другое.

Для того, чтобы получить количество `unicode`-символов в строке существует метод `public int codePointCount(int beginIndex, int endIndex)`:

```java
public class Test {
    public static void main(String[] args) {
        String literal = "\uD835\uDD0A";
        System.out.println(literal);
        System.out.println(literal.length());
        System.out.println(literal.codePointCount(0, literal.length()));
    }
}
```

Что выведет на экран уже:

```java
𝔊
2
1
```

> Помните!
> 
> Количество `char` символов в строке не всегда равно количеству `unicode` символов в ней!

### Туда и Обратно

#### Туда

Для перевода строки в последовательность байт в `Java` у `java.lang.String` существуют специальные методы:

```java
public byte[] getBytes()
public byte[] getBytes(Charset charset)
public byte[] getBytes(String charsetName) throws UnsupportedEncodingException
```

Кодировку можно явно передать по имени и получить байты, закодированные в ней:

```java
        String hello = "Hello World";
        byte[] utf8bytes = hello.getBytes("UTF-8");

        System.out.println(hello);
        System.out.println(Arrays.toString(utf8bytes));
```

Что выведет на экран:

```java
Hello World
[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]
```

> При передаче кодировки явно по имени необходимо также обработать `checked`-исключение `java.io.UnsupportedEncodingException`

Можно передать экземпляр класса `java.nio.charset.Charset`, самые популярные кодировки уже представлены в классе `java.nio.charset.StandardCharsets`:

```java
        String hello = "Hello World";
        byte[] utf8bytes = hello.getBytes(StandardCharsets.UTF_8);

        System.out.println(hello);
        System.out.println(Arrays.toString(utf8bytes));
```

Что выведет на экран:

```java
Hello World
[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]
```

Либо воспользоваться методом, который закодирует строку с помощью кодировки по-умолчанию для текущей платформы:

```java
        String hello = "Hello World";
        byte[] utf8bytes = hello.getBytes();

        System.out.println(hello);
        System.out.println(Arrays.toString(utf8bytes));
```

Что снова выведет на экран:

```java
Hello World
[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]
```

Узнать кодировку для платформы можно так:

```java
        System.out.println(Charset.defaultCharset());
```

Что для меня выведет:

```java
UTF-8
```

Я считаю, что лучше явно задавать кодировку, в которой вы хотите закодировать вашу последовательность символов, так как `defaultCharset` для различных платформ может быть разный.

#### Обратно

Для получения строки из последовательности байтов стоит воспользоваться конструктороми `java.lang.String`:

```java
public String(char value[])
public String(byte bytes[], Charset charset)
public String(byte bytes[], String charsetName) throws UnsupportedEncodingException
```

Принцип работы здесь ровно тот же, что описано в предыдущем блоке.

Из всего вышесказанного ясно, что даже когда **кажется**, что никто не использует явно кодировку, то **какая-то** кодировка все равно будет выбрана.

Это же касается и работы с файлами.

Кодировка либо задается явно, либо используется по-умолчанию.

Кодировка по-умолчанию зависит от платформы, на которой запущен код, и параметров запуска. На разных операционных системах и разных `JVM` может быть выбрана разная кодировка по-умолчанию.

## Заключение

Кодировка `UTF-8` является универсальной и наиболее популярной кодировкой на сегодняшний день.

В `Java` строки состоят из последовательностей символов, при этом тип данных `char` занимает 2 байта.
В `Unicode` существуют символы, для хранения которых необходимо более 2 байт, в таком случае такой символ будет представлен как два `char`-символа.

Наиболее популярные кодировки представлены объектами класса `java.nio.charset.Charset` и хранятся в `java.nio.charset.StandardCharsets`.

Класс `java.lang.String` предоставляет все необходимые методы для преобразования символов в байты и наоборот, при этом используемая кодировка будет либо указана явно, либо использована по-умолчанию.

Кодировка по-умолчанию может быть переопределена, но в основном будет использована `UTF-8`.

## Полезные ссылки

1. [Кодировки в Java](http://illegalargumentexception.blogspot.com/2009/05/java-rough-guide-to-character-encoding.html)
2. [Таблица символов Unicode](https://unicode-table.com/ru/)
3. [Unicode](https://ru.wikipedia.org/wiki/%D0%AE%D0%BD%D0%B8%D0%BA%D0%BE%D0%B4)